[
  {
    "title": "Desirability of outcome ranking (DOOR) analysis for multivariate survival outcomes with application to ACTT-1 trial",
    "abstract": "Desirability Of Outcome Ranking (DOOR) methodology accounts for problems that conventional benefit:risk analyses in clinical trials ignore, such as competing risks and the trade-off relationship between efficacy and toxicity. DOOR levels can be considered as a multi-state process in nature, as event-free survival, and survival with side effects are not equivalent and the overall patient trajectory requires recognition. In monotone settings where patients' conditions can only decline, we can record event times for each transition from one level of the DOOR to another, and construct Kaplan-Meier curves displaying transition times. While traditional survival analysis methods such as the Cox model require assumptions like proportional hazards and suffer from the challenge of interpreting a hazard ratio, Restricted Mean Survival Time (RMST) offers an alternative with greater intuitiveness. Therefore, we propose a combination of the two domains to develop estimation and inferential procedures that could benefit from the advantages of both DOOR and RMST. Particularly, the area under each survival curve restricted to a time point, or the RMST, has clear clinical meanings, from expected event-free survival time, expected survival time with at most one of the events, to expected lifetime before death. We show that the nonparametric estimator of the RMSTs asymptotically follows a multivariate Gaussian process through the martingale theory and functional delta method. There are alternative approaches to hypothesis testing that recognize when patients transition into worse states. We evaluate our proposed method with data simulated under a multistate model. We consider various scenarios, including when the null hypothesis is true, when the treatment difference exists only in certain DOOR levels, and small-sample studies. We also present a real-world example with ACTT-1."
  },
  {
    "title": "SHIELD: Safety on Humanoids via CBFs In Expectation on Learned Dynamics",
    "abstract": "Robot learning has produced remarkably effective ``black-box'' controllers for complex tasks such as dynamic locomotion on humanoids. Yet ensuring dynamic safety, i.e., constraint satisfaction, remains challenging for such policies. Reinforcement learning (RL) embeds constraints heuristically through reward engineering, and adding or modifying constraints requires retraining. Model-based approaches, like control barrier functions (CBFs), enable runtime constraint specification with formal guarantees but require accurate dynamics models. This paper presents SHIELD, a layered safety framework that bridges this gap by: (1) training a generative, stochastic dynamics residual model using real-world data from hardware rollouts of the nominal controller, capturing system behavior and uncertainties; and (2) adding a safety layer on top of the nominal (learned locomotion) controller that leverages this model via a stochastic discrete-time CBF formulation enforcing safety constraints in probability. The result is a minimally-invasive safety layer that can be added to the existing autonomy stack to give probabilistic guarantees of safety that balance risk and performance. In hardware experiments on an Unitree G1 humanoid, SHIELD enables safe navigation (obstacle avoidance) through varied indoor and outdoor environments using a nominal (unknown) RL controller and onboard perception."
  },
  {
    "title": "GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing",
    "abstract": "Editing images using natural language instructions has become a natural and expressive way to modify visual content; yet, evaluating the performance of such models remains challenging. Existing evaluation approaches often rely on image-text similarity metrics like CLIP, which lack precision. In this work, we introduce a new benchmark designed to evaluate text-guided image editing models in a more grounded manner, along two critical dimensions: (i) functional correctness, assessed via automatically generated multiple-choice questions that verify whether the intended change was successfully applied; and (ii) image content preservation, which ensures that non-targeted regions of the image remain visually consistent using an object-aware masking technique and preservation scoring. The benchmark includes over 1000 high-quality editing examples across 20 diverse content categories, each annotated with detailed editing instructions, evaluation questions, and spatial object masks. We conduct a large-scale study comparing GPT-Image-1, the latest flagship in the text-guided image editing space, against several state-of-the-art editing models, and validate our automatic metrics against human ratings. Results show that GPT-Image-1 leads in instruction-following accuracy, but often over-modifies irrelevant image regions, highlighting a key trade-off in the current model behavior. GIE-Bench provides a scalable, reproducible framework for advancing more accurate evaluation of text-guided image editing."
  },
  {
    "title": "Duality for finitely valued algebras",
    "abstract": "The theory of natural dualities provides a well-developed framework for studying Stone-like dualities induced by an algebra $\\mathbf{L}$ which acts as a dualizing object when equipped with suitable topological and relational structure. The development of this theory has, however, largely remained restricted to the case where $\\mathbf{L}$ is finite. Motivated by the desire to provide a universal algebraic formulation of the existing duality of Cignoli and Marra or locally weakly finite MV-algebras and to extend it to a corresponding class of positive MV-algebras, in this paper we investigate Stone-like dualities where the algebra $\\mathbf{L}$ is allowed to be infinite. This requires restricting our attention from the whole prevariety generated by $\\mathbf{L}$ to the subclass of algebras representable as algebras of $\\mathbf{L}$-valued functions of finite range, a distinction that does not arise in the case of finite $\\mathbf{L}$. Provided some requirements on $\\mathbf{L}$ are met, our main result establishes a categorical duality for this class of algebras, which covers the above cases of MV-algebras and positive MV-algebras."
  },
  {
    "title": "Unsupervised Detection of Distribution Shift in Inverse Problems using Diffusion Models",
    "abstract": "Diffusion models are widely used as priors in imaging inverse problems. However, their performance often degrades under distribution shifts between the training and test-time images. Existing methods for identifying and quantifying distribution shifts typically require access to clean test images, which are almost never available while solving inverse problems (at test time). We propose a fully unsupervised metric for estimating distribution shifts using only indirect (corrupted) measurements and score functions from diffusion models trained on different datasets. We theoretically show that this metric estimates the KL divergence between the training and test image distributions. Empirically, we show that our score-based metric, using only corrupted measurements, closely approximates the KL divergence computed from clean images. Motivated by this result, we show that aligning the out-of-distribution score with the in-distribution score -- using only corrupted measurements -- reduces the KL divergence and leads to improved reconstruction quality across multiple inverse problems."
  },
  {
    "title": "Improving Assembly Code Performance with Large Language Models via Reinforcement Learning",
    "abstract": "Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance."
  },
  {
    "title": "Fractal geometry predicts dynamic differences in structural and functional connectomes",
    "abstract": "Understanding the intricate architecture of brain networks and its connection to brain function is essential for deciphering the underlying principles of cognition and disease. While traditional graph-theoretical measures have been widely used to characterize these networks, they often fail to fully capture the emergent properties of large-scale neural dynamics. Here, we introduce an alternative approach to quantify brain networks that is rooted in complex dynamics, fractal geometry, and asymptotic analysis. We apply these concepts to brain connectomes and demonstrate how quadratic iterations and geometric properties of Mandelbrot-like sets can provide novel insights into structural and functional network dynamics. Our findings reveal fundamental distinctions between structural (positive) and functional (signed) connectomes, such as the shift of cusp orientation and the variability in equi-M set geometry. Notably, structural connectomes exhibit more robust, predictable features, while functional connectomes show increased variability for non-trivial tasks. We further demonstrate that traditional graph-theoretical measures, when applied separately to the positive and negative sub-networks of functional connectomes, fail to fully capture their dynamic complexity. Instead, size and shape-based invariants of the equi-M set effectively differentiate between rest and emotional task states, which highlights their potential as superior markers of emergent network dynamics. These results suggest that incorporating fractal-based methods into network neuroscience provides a powerful tool for understanding how information flows in natural systems beyond static connectivity measures, while maintaining their simplicity."
  },
  {
    "title": "Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks",
    "abstract": "Classic multi-agent reinforcement learning (MARL) methods require that agents enjoy global state observability, preventing development of decentralized algorithms and limiting scalability. Recent work has shown that, under assumptions on decaying inter-agent influence, global observability can be replaced by local neighborhood observability at each agent, enabling decentralization and scalability. Real-world applications enjoying such decay properties remain underexplored, however, despite the fact that signal power decay, or signal attenuation, due to path loss is an intrinsic feature of many problems in wireless communications and radar networks. In this paper, we show that signal attenuation enables decentralization in MARL by considering the illustrative special case of performing power allocation for target detection in a radar network. To achieve this, we propose two new constrained multi-agent Markov decision process formulations of this power allocation problem, derive local neighborhood approximations for global value function and gradient estimates and establish corresponding error bounds, and develop decentralized saddle point policy gradient algorithms for solving the proposed problems. Our approach, though oriented towards the specific radar network problem we consider, provides a useful model for future extensions to additional problems in wireless communications and radar networks."
  },
  {
    "title": "Unsolvability and Beyond in Many-To-Many Non-Bipartite Stable Matching",
    "abstract": "We study the Stable Fixtures problem, a many-to-many generalisation of the classical non-bipartite Stable Roommates matching problem. Building on the foundational work of Tan on stable partitions, we extend his results to this significantly more general setting and develop a rich framework for understanding stable structures in many-to-many contexts. Our main contribution, the notion of a generalised stable partition (GSP), not only characterises the solution space of this problem, but also serves as a versatile tool for reasoning about ordinal preference systems with capacity constraints. We show that GSPs can be computed efficiently and provide an elegant representation of a problem instance, tightly characterising its preference structure and succinctly certifying the existence and non-existence of stable matchings. Leveraging a connection to stable half matchings, we also establish a non-bipartite analogue of the Rural Hospitals Theorem for stable half-matchings and GSPs, and connect our results to recent work on near-feasible matchings, providing a simpler algorithm and tighter analysis for this problem. Our work also addresses the computational challenges of finding optimal stable half-matchings and GSPs, presenting a flexible integer linear programming model for various objectives. Beyond theoretical insights, we conduct the first empirical analysis of random Stable Fixtures instances, uncovering surprising results, such as the impact of capacity functions on the solvability likelihood. Our work not only unifies and extends classical and recent perspectives on stability in non-bipartite stable matching but also establishes new tools, techniques, and directions for advancing the study of stable matchings and their applications."
  },
  {
    "title": "EMU/GAMA: A new approach to characterising radio luminosity functions",
    "abstract": "This study characterises the radio luminosity functions (RLFs) for SFGs and AGN using statistical redshift estimation in the absence of comprehensive spectroscopic data. Sensitive radio surveys over large areas detect many sources with faint optical and infrared counterparts, for which redshifts and spectra are unavailable. This challenges our attempt to understand the population of radio sources. Statistical tools are often used to model parameters (such as redshift) as an alternative to observational data. Using the data from GAMA G23 and EMU early science observations, we explore simple statistical techniques to estimate the redshifts in order to measure the RLFs of the G23 radio sources as a whole and for SFGs and AGN separately. Redshifts and AGN/SFG classifications are assigned statistically for those radio sources without spectroscopic data. The calculated RLFs are compared with existing studies, and the results suggest that the RLFs match remarkably well for low redshift galaxies with an optical counterpart. We use a more realistic high redshift distribution to model the redshifts of (most likely) high redshift radio sources and find that the LFs from our approach match well with measured LFs. We also look at strategies to compare the RLFs of radio sources without an optical counterpart to existing studies."
  },
  {
    "title": "Coherent Spectroscopic Probes of Topology: A Velocity-Gauge Perspective",
    "abstract": "We present a velocity-gauge formalism for computing nonlinear current response functions in periodic systems and apply it to the Su-Schrieffer-Heeger (SSH) model as a minimal topological testbed. By retaining the full minimal coupling Hamiltonian and avoiding the rotating wave approximation, we construct gauge-consistent expressions for the linear and third-order current susceptibilities using retarded Green's functions. Our results reveal how nonlinear optical spectra encode not only energy-level transitions but also interband phase coherence and topological winding. In the topological phase, the third-order response exhibits characteristic phase inversions and spectral asymmetries that are absent in the trivial phase. These features reflect geometric changes in the Bloch eigenstates and highlight the role of virtual pathways in shaping the nonlinear signal. Our framework offers a robust and extensible platform for modeling nonlinear light-matter interactions in topological materials beyond the dipole approximation and the standard Coulomb-gauge formulation in molecular spectroscopy."
  },
  {
    "title": "Reconfigurable Integrated Photonic Chips as Dual-Purpose Neuromorphic Accelerators and Physical Unclonable Functions",
    "abstract": "In this work, we experimentally validate the dual use of a reconfigurable photonic integrated mesh as a neuromorphic accelerator, targeting signal equalization, and as a physical unclonable function offering authentication at the hardware level. The processing node is an optical spectrum slicing self-coherent transceiver targeting the mitigation of dispersion impairments of an intensity detected QPSK signal, after 25 km of transmission at 32 Gbaud. Unavoidable fabrication related imperfections of the nodes, such as waveguide roughness, can act as fingerprints of the device, and, during neuromorphic processing, result in unique weights at the digital back-end during signal equalization. Extracted security metrics offer low false positive/negative probability for the generated responses, confirming un-clonability, whereas bit-error-ratio for the QPSK equalization task was always below the hardware forward error correction limit. The experimental results substantiate the capability of the proposed scheme to simultaneously act as an accelerator and as a security token."
  },
  {
    "title": "Combined Experimental and Computational Analysis of Lithium Diffusion in Isostructural Pair VNb9O25 and VTa9O25",
    "abstract": "Wadsley-Roth crystal structures are an attractive class of materials for batteries because lithium diffusion is facilitated by the ReO3-like block structure with electron transport enabled by edge-sharing along shear planes. However, clear structure-property relationships remain limited, making it challenging to develop improved materials. Here, the first lithiation of VTa9O25 is reported, enabling a direct isostructural comparison with the better-known VNb9O25. These materials have similar unit cell volumes and atomic radii yet exhibit different voltage windows, C-rate dependent capacities, and transport metrics. Time-dependent overpotential analysis reveals ionic diffusion as the primary bottleneck to high rate-performance in both cases, however, the lithium diffusivity for VNb9O25 was an order of magnitude faster than that for VTa9O25. These experimental trends aligned well with density functional theory calculations combined with molecular dynamics that show a factor of six faster diffusion in VNb9O25. Nudged elastic band calculations of the probable hopping pathways indicate that VNb9O25 consistently exhibits a lower activation barrier for lithium diffusion. Bader charge analysis reveals a larger net charge on Li in VNb9O25 due to the higher electronegativity of Nb which stabilizes the transition state and lowers the barrier. This stabilization arises from the stronger Coulombic interaction between Li and its coordinated O-environment. These materials behave similarly upon lithiation wherein the lattice vectors (corresponding to the block plane) increase until about 50% lithiation and then decrease. However, the electronic structure differs, indicating that VNb9O25 undergoes a insulator to metal transition at a lower state of charge compared with VTa9O25. Overall, this work establishes the role of the cation (Nb or Ta) on the electronic and transport properties during lithiation."
  },
  {
    "title": "Neural-Network Correlation Functions for Light Nuclei with Chiral Two- and Three-Body Interactions",
    "abstract": "Finding high-quality trial wave functions for quantum Monte Carlo calculations of light nuclei requires a strong intuition for modeling the interparticle correlations as well as large computational resources for exploring the space of variational parameters. Moreover, for systems with three-body interactions, the wave function should account for many-body effects beyond simple pairwise correlations. In this work, we design neural networks that efficiently incorporate these factors to generate expressive wave function Ansatzes for light nuclei using variational Monte Carlo. Our neural-network approach for A=3 nuclei can capture, already at the level of variational Monte Carlo, the overwhelming majority of the ground-state energy estimated by Green's Function Monte Carlo (GFMC). We can find a 91% improvement over standard variational Monte Carlo and achieve a ground state energy within 0.45% of the GFMC result for 3H using the softest chiral interaction with neural networks. The result indicates the potential of neural networks to construct effective trial wave functions for quantum Monte Carlo calculations."
  },
  {
    "title": "General superconvergence for kernel-based approximation",
    "abstract": "Kernel interpolation is a fundamental technique for approximating functions from scattered data, with a well-understood convergence theory when interpolating elements of a reproducing kernel Hilbert space. Beyond this classical setting, research has focused on two regimes: misspecified interpolation, where the kernel smoothness exceeds that of the target function, and superconvergence, where the target is smoother than the Hilbert space. This work addresses the latter, where smoother target functions yield improved convergence rates, and extends existing results by characterizing superconvergence for projections in general Hilbert spaces. We show that functions lying in ranges of certain operators, including adjoint of embeddings, exhibit accelerated convergence, which we extend across interpolation scales between these ranges and the full Hilbert space. In particular, we analyze Mercer operators and embeddings into $L_p$ spaces, linking the images of adjoint operators to Mercer power spaces. Applications to Sobolev spaces are discussed in detail, highlighting how superconvergence depends critically on boundary conditions. Our findings generalize and refine previous results, offering a broader framework for understanding and exploiting superconvergence. The results are supported by numerical experiments."
  },
  {
    "title": "Controlling the Flow: Stability and Convergence for Stochastic Gradient Descent with Decaying Regularization",
    "abstract": "The present article studies the minimization of convex, L-smooth functions defined on a separable real Hilbert space. We analyze regularized stochastic gradient descent (reg-SGD), a variant of stochastic gradient descent that uses a Tikhonov regularization with time-dependent, vanishing regularization parameter. We prove strong convergence of reg-SGD to the minimum-norm solution of the original problem without additional boundedness assumptions. Moreover, we quantify the rate of convergence and optimize the interplay between step-sizes and regularization decay. Our analysis reveals how vanishing Tikhonov regularization controls the flow of SGD and yields stable learning dynamics, offering new insights into the design of iterative algorithms for convex problems, including those that arise in ill-posed inverse problems. We validate our theoretical findings through numerical experiments on image reconstruction and ODE-based inverse problems."
  },
  {
    "title": "Is Grokking a Computational Glass Relaxation?",
    "abstract": "Understanding neural network's (NN) generalizability remains a central question in deep learning research. The special phenomenon of grokking, where NNs abruptly generalize long after the training performance reaches a near-perfect level, offers a unique window to investigate the underlying mechanisms of NNs' generalizability. Here we propose an interpretation for grokking by framing it as a computational glass relaxation: viewing NNs as a physical system where parameters are the degrees of freedom and train loss is the system energy, we find memorization process resembles a rapid cooling of liquid into non-equilibrium glassy state at low temperature and the later generalization is like a slow relaxation towards a more stable configuration. This mapping enables us to sample NNs' Boltzmann entropy (states of density) landscape as a function of training loss and test accuracy. Our experiments in transformers on arithmetic tasks suggests that there is NO entropy barrier in the memorization-to-generalization transition of grokking, challenging previous theory that defines grokking as a first-order phase transition. We identify a high-entropy advantage under grokking, an extension of prior work linking entropy to generalizability but much more significant. Inspired by grokking's far-from-equilibrium nature, we develop a toy optimizer WanD based on Wang-landau molecular dynamics, which can eliminate grokking without any constraints and find high-norm generalizing solutions. This provides strictly-defined counterexamples to theory attributing grokking solely to weight norm evolution towards the Goldilocks zone and also suggests new potential ways for optimizer design."
  },
  {
    "title": "Diff-Unfolding: A Model-Based Score Learning Framework for Inverse Problems",
    "abstract": "Diffusion models are extensively used for modeling image priors for inverse problems. We introduce \\emph{Diff-Unfolding}, a principled framework for learning posterior score functions of \\emph{conditional diffusion models} by explicitly incorporating the physical measurement operator into a modular network architecture. Diff-Unfolding formulates posterior score learning as the training of an unrolled optimization scheme, where the measurement model is decoupled from the learned image prior. This design allows our method to generalize across inverse problems at inference time by simply replacing the forward operator without retraining. We theoretically justify our unrolling approach by showing that the posterior score can be derived from a composite model-based optimization formulation. Extensive experiments on image restoration and accelerated MRI show that Diff-Unfolding achieves state-of-the-art performance, improving PSNR by up to 2 dB and reducing LPIPS by $22.7\\%$, while being both compact (47M parameters) and efficient (0.72 seconds per $256 \\times 256$ image). An optimized C++/LibTorch implementation further reduces inference time to 0.63 seconds, underscoring the practicality of our approach."
  },
  {
    "title": "The Poisson Multiplication Formula",
    "abstract": "We establish necessary and sufficient conditions implying that the product of $m\\geq 2$ Poisson functionals, living in a finite sum of Wiener chaoses, is square-integrable. Our conditions are expressed in terms of iterated add-one cost operators, and are obtained through the use of a novel family of Poincar\\'e inequalities for almost surely finite random variables, generalizing the recent findings by Trauthwein (2024). When specialized to the case of multiple Wiener-It\\^o integrals, our results yield general multiplication formulae on the Poisson space under minimal conditions, naturally expressed in terms of partitions and diagrams. Our work addresses several questions left open in a seminal work by Surgailis (1984), and completes a line of research initiated in D\\\"obler and Peccati (2018)."
  },
  {
    "title": "Anti-aliasing of neural distortion effects via model fine tuning",
    "abstract": "Neural networks have become ubiquitous with guitar distortion effects modelling in recent years. Despite their ability to yield perceptually convincing models, they are susceptible to frequency aliasing when driven by high frequency and high gain inputs. Nonlinear activation functions create both the desired harmonic distortion and unwanted aliasing distortion as the bandwidth of the signal is expanded beyond the Nyquist frequency. Here, we present a method for reducing aliasing in neural models via a teacher-student fine tuning approach, where the teacher is a pre-trained model with its weights frozen, and the student is a copy of this with learnable parameters. The student is fine-tuned against an aliasing-free dataset generated by passing sinusoids through the original model and removing non-harmonic components from the output spectra. Our results show that this method significantly suppresses aliasing for both long-short-term-memory networks (LSTM) and temporal convolutional networks (TCN). In the majority of our case studies, the reduction in aliasing was greater than that achieved by two times oversampling. One side-effect of the proposed method is that harmonic distortion components are also affected. This adverse effect was found to be model-dependent, with the LSTM models giving the best balance between anti-aliasing and preserving the perceived similarity to an analog reference device."
  },
  {
    "title": "Understanding Nonlinear Implicit Bias via Region Counts in Input Space",
    "abstract": "One explanation for the strong generalization ability of neural networks is implicit bias. Yet, the definition and mechanism of implicit bias in non-linear contexts remains little understood. In this work, we propose to characterize implicit bias by the count of connected regions in the input space with the same predicted label. Compared with parameter-dependent metrics (e.g., norm or normalized margin), region count can be better adapted to nonlinear, overparameterized models, because it is determined by the function mapping and is invariant to reparametrization. Empirically, we found that small region counts align with geometrically simple decision boundaries and correlate well with good generalization performance. We also observe that good hyper-parameter choices such as larger learning rates and smaller batch sizes can induce small region counts. We further establish the theoretical connections and explain how larger learning rate can induce small region counts in neural networks."
  },
  {
    "title": "Compact, Large-Scale Photonic Neurons by Modulation-and-Weight Microring Resonators",
    "abstract": "Fabrication imperfections, spatial constraints, and prohibitive costs collectively impede the scalability of neuromorphic photonics. In this work, we introduce a large-scale, compact photonic neuron in which each microring performs modulation and weighting simultaneously. This dual functionality is realized by leveraging both the carrier effect and thermal tunability, thereby merging modulation and weighting to conserve on-chip area, enhancing tuning efficiency, and capitalizing on wavelength-division multiplexing (WDM) for scalable implementations. In addition, we investigated a range of configurations for the proposed neuron to better tailor its behavior to various computational tasks. To illustrate the adaptability of the system's tasks, we explore both spatial and temporal domains, highlighting its versatility through two representative tasks: image processing and, for the first time, financial time series analysis, which represents a promising new frontier for neuromorphic photonics. These findings underscore the considerable promise of photonic computing in addressing a breadth of real-world challenges, particularly under escalating demands for both scalability and flexibility."
  },
  {
    "title": "Learning Multimodal AI Algorithms for Amplifying Limited User Input into High-dimensional Control Space",
    "abstract": "Current invasive assistive technologies are designed to infer high-dimensional motor control signals from severely paralyzed patients. However, they face significant challenges, including public acceptance, limited longevity, and barriers to commercialization. Meanwhile, noninvasive alternatives often rely on artifact-prone signals, require lengthy user training, and struggle to deliver robust high-dimensional control for dexterous tasks. To address these issues, this study introduces a novel human-centered multimodal AI approach as intelligent compensatory mechanisms for lost motor functions that could potentially enable patients with severe paralysis to control high-dimensional assistive devices, such as dexterous robotic arms, using limited and noninvasive inputs. In contrast to the current state-of-the-art (SoTA) noninvasive approaches, our context-aware, multimodal shared-autonomy framework integrates deep reinforcement learning algorithms to blend limited low-dimensional user input with real-time environmental perception, enabling adaptive, dynamic, and intelligent interpretation of human intent for complex dexterous manipulation tasks, such as pick-and-place. The results from our ARAS (Adaptive Reinforcement learning for Amplification of limited inputs in Shared autonomy) trained with synthetic users over 50,000 computer simulation episodes demonstrated the first successful implementation of the proposed closed-loop human-in-the-loop paradigm, outperforming the SoTA shared autonomy algorithms. Following a zero-shot sim-to-real transfer, ARAS was evaluated on 23 human subjects, demonstrating high accuracy in dynamic intent detection and smooth, stable 3D trajectory control for dexterous pick-and-place tasks. ARAS user study achieved a high task success rate of 92.88%, with short completion times comparable to those of SoTA invasive assistive technologies."
  },
  {
    "title": "Upper moderate deviation probabilities for the maximum of branching Brownian motion",
    "abstract": "It is known from~\\cite{Bramson1983} that the maximum of branching Brownian motion at time $t$ is asymptotically around an explicit function $m_t$, which involves a first ballistic order and a logarithmic correction. In this paper, we give an asymptotic equivalent for its upper moderate deviation probability, that is, the probability that the maximum achieves $m_t + x_t$ at time $t$, where $1 \\ll x_t \\ll t$. We adopt a probabilistic approach that employs a modified version of the second moment method."
  },
  {
    "title": "LGBQPC: Local Granular-Ball Quality Peaks Clustering",
    "abstract": "The density peaks clustering (DPC) algorithm has attracted considerable attention for its ability to detect arbitrarily shaped clusters based on a simple yet effective assumption. Recent advancements integrating granular-ball (GB) computing with DPC have led to the GB-based DPC (GBDPC) algorithm, which improves computational efficiency. However, GBDPC demonstrates limitations when handling complex clustering tasks, particularly those involving data with complex manifold structures or non-uniform density distributions. To overcome these challenges, this paper proposes the local GB quality peaks clustering (LGBQPC) algorithm, which offers comprehensive improvements to GBDPC in both GB generation and clustering processes based on the principle of justifiable granularity (POJG). Firstly, an improved GB generation method, termed GB-POJG+, is developed, which systematically refines the original GB-POJG in four key aspects: the objective function, termination criterion for GB division, definition of abnormal GB, and granularity level adaptation strategy. GB-POJG+ simplifies parameter configuration by requiring only a single penalty coefficient and ensures high-quality GB generation while maintaining the number of generated GBs within an acceptable range. In the clustering phase, two key innovations are introduced based on the GB k-nearest neighbor graph: relative GB quality for density estimation and geodesic distance for GB distance metric. These modifications substantially improve the performance of GBDPC on datasets with complex manifold structures or non-uniform density distributions. Extensive numerical experiments on 40 benchmark datasets, including both synthetic and publicly available datasets, validate the superior performance of the proposed LGBQPC algorithm."
  },
  {
    "title": "STRIDE: Sparse Techniques for Regression in Deep Gaussian Processes",
    "abstract": "Gaussian processes (GPs) have gained popularity as flexible machine learning models for regression and function approximation with an in-built method for uncertainty quantification. However, GPs suffer when the amount of training data is large or when the underlying function contains multi-scale features that are difficult to represent by a stationary kernel. To address the former, training of GPs with large-scale data is often performed through inducing point approximations (also known as sparse GP regression (GPR)), where the size of the covariance matrices in GPR is reduced considerably through a greedy search on the data set. To aid the latter, deep GPs have gained traction as hierarchical models that resolve multi-scale features by combining multiple GPs. Posterior inference in deep GPs requires a sampling or, more usual, a variational approximation. Variational approximations lead to large-scale stochastic, non-convex optimisation problems and the resulting approximation tends to represent uncertainty incorrectly. In this work, we combine variational learning with MCMC to develop a particle-based expectation-maximisation method to simultaneously find inducing points within the large-scale data (variationally) and accurately train the GPs (sampling-based). The result is a highly efficient and accurate methodology for deep GP training on large-scale data. We test our method on standard benchmark problems."
  },
  {
    "title": "Revisiting Stochastic Approximation and Stochastic Gradient Descent",
    "abstract": "In this paper, we take a fresh look at stochastic approximation (SA) and Stochastic Gradient Descent (SGD). We derive new sufficient conditions for the convergence of SA. In particular, the \"noise\" or measurement error need not have a finite second moment, and under suitable conditions, not even a finite mean. By adapting this method of proof, we also derive sufficient conditions for the convergence of zero-order SGD, wherein the stochastic gradient is computed using only two function evaluations, and no gradient computations. The sufficient conditions derived here are the weakest to date, thus leading to a considerable expansion of the applicability of SA and SGD theory."
  },
  {
    "title": "DecompileBench: A Comprehensive Benchmark for Evaluating Decompilers in Real-World Scenarios",
    "abstract": "Decompilers are fundamental tools for critical security tasks, from vulnerability discovery to malware analysis, yet their evaluation remains fragmented. Existing approaches primarily focus on syntactic correctness through synthetic micro-benchmarks or subjective human ratings, failing to address real-world requirements for semantic fidelity and analyst usability. We present DecompileBench, the first comprehensive framework that enables effective evaluation of decompilers in reverse engineering workflows through three key components: \\textit{real-world function extraction} (comprising 23,400 functions from 130 real-world programs), \\textit{runtime-aware validation}, and \\textit{automated human-centric assessment} using LLM-as-Judge to quantify the effectiveness of decompilers in reverse engineering workflows. Through a systematic comparison between six industrial-strength decompilers and six recent LLM-powered approaches, we demonstrate that LLM-based methods surpass commercial tools in code understandability despite 52.2% lower functionality correctness. These findings highlight the potential of LLM-based approaches to transform human-centric reverse engineering. We open source \\href{https://github.com/Jennieett/DecompileBench}{DecompileBench} to provide a framework to advance research on decompilers and assist security experts in making informed tool selections based on their specific requirements."
  },
  {
    "title": "Palladium: A DPU-enabled Multi-Tenant Serverless Cloud over Zero-copy Multi-node RDMA Fabrics",
    "abstract": "Serverless computing promises enhanced resource efficiency and lower user costs, yet is burdened by a heavyweight, CPU-bound data plane. Prior efforts exploiting shared memory reduce overhead locally but fall short when scaling across nodes. Furthermore, serverless environments can have unpredictable and large-scale multi-tenancy, leading to contention for shared network resources. We present Palladium, a DPU-centric serverless data plane that reduces the CPU burden and enables efficient, zero-copy communication in multi-tenant serverless clouds. Despite the limited general-purpose processing capability of the DPU cores, Palladium strategically exploits the DPU's potential by (1) offloading data transmission to high-performance NIC cores via RDMA, combined with intra-node shared memory to eliminate data copies across nodes, and (2) enabling cross-processor (CPU-DPU) shared memory to eliminate redundant data movement, which overwhelms wimpy DPU cores. At the core of Palladium is the DPU-enabled network engine (DNE) -- a lightweight reverse proxy that isolates RDMA resources from tenant functions, orchestrates inter-node RDMA flows, and enforces fairness under contention. To further reduce CPU involvement, Palladium performs early HTTP/TCP-to-RDMA transport conversion at the cloud ingress, bridging the protocol mismatch before client traffic enters the RDMA fabric, thus avoiding costly protocol translation along the critical path. We show that careful selection of RDMA primitives (i.e., two-sided instead of one-sided) significantly affects the zero-copy data plane. Our preliminary experimental results show that enabling DPU offloading in Palladium improves RPS by 20.9x. The latency is reduced by a factor of 21x in the best case, all the while saving up to 7 CPU cores, and only consuming two wimpy DPU cores."
  },
  {
    "title": "Features of the Partition Function of a $\u039b>0$ Universe",
    "abstract": "We consider properties of the gravitational path integral, ${Z}_{\\text{grav}}$, of a four-dimensional gravitational effective field theory with $\\Lambda>0$ at the quantum level. To leading order, ${Z}_{\\text{grav}}$ is dominated by a four-sphere saddle subject to small fluctuations. Beyond this, ${Z}_{\\text{grav}}$ receives contributions from additional geometries that may include Einstein metrics of positive curvature. We discuss how a general positive curvature Einstein metric contributes to ${Z}_{\\text{grav}}$ at one-loop level. Along the way, we discuss Einstein-Maxwell theory with $\\Lambda>0$, and identify an interesting class of closed non-Einstein gravitational instantons. We provide a detailed study for the specific case of $\\mathbb{C}P^2$ which is distinguished as the saddle with second largest volume and positive definite tensor eigenspectrum. We present exact one-loop results for scalar particles, Maxwell theory, and Einstein gravity about the Fubini-Study metric on $\\mathbb{C}P^2$."
  },
  {
    "title": "Convergence Rates of Constrained Expected Improvement",
    "abstract": "Constrained Bayesian optimization (CBO) methods have seen significant success in black-box optimization with constraints, and one of the most commonly used CBO methods is the constrained expected improvement (CEI) algorithm. CEI is a natural extension of the expected improvement (EI) when constraints are incorporated. However, the theoretical convergence rate of CEI has not been established. In this work, we study the convergence rate of CEI by analyzing its simple regret upper bound. First, we show that when the objective function $f$ and constraint function $c$ are assumed to each lie in a reproducing kernel Hilbert space (RKHS), CEI achieves the convergence rates of $\\mathcal{O} \\left(t^{-\\frac{1}{2}}\\log^{\\frac{d+1}{2}}(t) \\right) \\ \\text{and }\\ \\mathcal{O}\\left(t^{\\frac{-\\nu}{2\\nu+d}} \\log^{\\frac{\\nu}{2\\nu+d}}(t)\\right)$ for the commonly used squared exponential and Mat\\'{e}rn kernels, respectively. Second, we show that when $f$ and $c$ are assumed to be sampled from Gaussian processes (GPs), CEI achieves the same convergence rates with a high probability. Numerical experiments are performed to validate the theoretical analysis."
  },
  {
    "title": "Understanding and Characterizing Obfuscated Funds Transfers in Ethereum Smart Contracts",
    "abstract": "Scam contracts on Ethereum have rapidly evolved alongside the rise of DeFi and NFT ecosystems, utilizing increasingly complex code obfuscation techniques to avoid early detection. This paper systematically investigates how obfuscation amplifies the financial risks of fraudulent contracts and undermines existing auditing tools. We propose a transfer-centric obfuscation taxonomy, distilling seven key features, and introduce ObfProbe, a framework that performs bytecode-level smart contract analysis to uncover obfuscation techniques and quantify obfuscation complexity via Z-score ranking. In a large-scale study of 1.03 million Ethereum contracts, we isolate over 3 000 highly obfuscated contracts and identify two scam archetypes, three high-risk contract categories, and MEV bots that employ a variety of obfuscation maneuvers such as inline assembly, dead code insertion, and deep function splitting. We further show that obfuscation substantially increases both the scale of financial damage and the time until detection. Finally, we evaluate SourceP, a state-of-the-art Ponzi detection tool, on obfuscated versus non-obfuscated samples and observe its accuracy drop from approximately 80 percent to approximately 12 percent in real-world scenarios. These findings highlight the urgent need for enhanced anti-obfuscation analysis techniques and broader community collaboration to stem the proliferation of scam contracts in the expanding DeFi ecosystem."
  },
  {
    "title": "On the Role of Weight Decay in Collaborative Filtering: A Popularity Perspective",
    "abstract": "Collaborative filtering (CF) enables large-scale recommendation systems by encoding information from historical user-item interactions into dense ID-embedding tables. However, as embedding tables grow, closed-form solutions become impractical, often necessitating the use of mini-batch gradient descent for training. Despite extensive work on designing loss functions to train CF models, we argue that one core component of these pipelines is heavily overlooked: weight decay. Attaining high-performing models typically requires careful tuning of weight decay, regardless of loss, yet its necessity is not well understood. In this work, we question why weight decay is crucial in CF pipelines and how it impacts training. Through theoretical and empirical analysis, we surprisingly uncover that weight decay's primary function is to encode popularity information into the magnitudes of the embedding vectors. Moreover, we find that tuning weight decay acts as a coarse, non-linear knob to influence preference towards popular or unpopular items. Based on these findings, we propose PRISM (Popularity-awaRe Initialization Strategy for embedding Magnitudes), a straightforward yet effective solution to simplify the training of high-performing CF models. PRISM pre-encodes the popularity information typically learned through weight decay, eliminating its necessity. Our experiments show that PRISM improves performance by up to 4.77% and reduces training times by 38.48%, compared to state-of-the-art training strategies. Additionally, we parameterize PRISM to modulate the initialization strength, offering a cost-effective and meaningful strategy to mitigate popularity bias."
  },
  {
    "title": "Thermal Static Potential and Pseudo-Scalar Quarkonium Spectral Functions from 2+1 Flavor Lattice QCD",
    "abstract": "Quarkonia, which are bound states of a heavy quark and antiquark, play a key role in probing the quark-gluon plasma (QGP). The dynamics of quarkonia in the QGP are encoded in their finite-temperature spectral functions. In this work, we estimate the quarkonium spectral functions in the pseudo-scalar channel using 2+1 flavor lattice QCD with a pion mass of $320\\,\\text{MeV}$, at temperatures of $220\\,\\text{MeV}\\,(1.2\\,T_{pc}),\\,251\\,\\text{MeV}\\,(1.4\\,T_{pc})\\,\\text{and}\\,293\\,\\text{MeV}\\,(1.6\\,T_{pc})$. Reconstructing the spectral function from the Euclidean lattice correlator is a well-known ill-posed problem, requiring additional physics-motivated input. We address this by smoothly matching contributions from different frequency regions of the spectral function, using appropriate physics valid for each region. The spectral function around $\\omega \\sim 2\\,M_q$ is obtained using a non-perturbative complex potential, while for $\\omega \\gg 2\\,M_q$ it is modeled using results from vacuum perturbation theory. Since the pseudoscalar channel does not receive a transport contribution near $\\omega \\sim 0$, we find that the combination of these two regions already provides a good description of the relativistic lattice pseudoscalar correlator. We observe a substantial thermal width in the $\\eta_c(1S)$ state, indicating that pseudoscalar charmonium ($\\eta_c$) is nearing dissolution at the studied temperatures. In comparison, the $\\eta_b$ ground state exhibits little change and remains well-defined."
  },
  {
    "title": "Measurements of $W^+W^-$ production cross-sections in $pp$ collisions at $\\sqrt{s}=13$ TeV with the ATLAS detector",
    "abstract": "Measurements of $W^+W^-\\rightarrow e^\\pm \\nu \\mu^\\mp \\nu$ production cross-sections are presented, providing a test of the predictions of perturbative quantum chromodynamics and the electroweak theory. The measurements are based on data from $pp$ collisions at $\\sqrt{s}=13$ TeV recorded by the ATLAS detector at the Large Hadron Collider in 2015-2018, corresponding to an integrated luminosity of 140 fb$^{-1}$. The number of events due to top-quark pair production, the largest background, is reduced by rejecting events containing jets with $b$-hadron decays. An improved methodology for estimating the remaining top-quark background enables a precise measurement of $W^+W^-$ cross-sections with no additional requirements on jets. The fiducial $W^+W^-$ cross-section is determined in a maximum-likelihood fit with an uncertainty of 3.1%. The measurement is extrapolated to the full phase space, resulting in a total $W^+W^-$ cross-section of $127\\pm4$ pb. Differential cross-sections are measured as a function of twelve observables that comprehensively describe the kinematics of $W^+W^-$ events. The measurements are compared with state-of-the-art theory calculations and excellent agreement with predictions is observed. A charge asymmetry in the lepton rapidity is observed as a function of the dilepton invariant mass, in agreement with the Standard Model expectation. A CP-odd observable is measured to be consistent with no CP violation. Limits on Standard Model effective field theory Wilson coefficients in the Warsaw basis are obtained from the differential cross-sections."
  },
  {
    "title": "Systematic analysis of double Gamow-Teller sum rules",
    "abstract": "Sum rules are important bulk properties of transition strength functions for atomic nuclei. Unlike the Ikeda sum rule for single Gamow-Teller transition, double Gamow-Teller transition sum rules rely on the details of many-body wavefunctions. We approximate the shell model ground state with nucleon-pair condensates, by projection after variation, and compute double Gamow-Teller (DGT) transition sum rules from both $\\beta+$ and $\\beta-$ directions. By systematic investigation of DGT sum rules of even-even nuclei in the $1s0d$, $1p0f$ major shells, we quantitatively estimate the model-dependent fractions in the sum rules, and analyze the importance of double isospin-analogue state in the DGT strength function."
  },
  {
    "title": "Prime Number Error Terms",
    "abstract": "In 1980 Montgomery made a conjecture about the true order of the error term in the prime number theorem. In 2012 the author made an analogous conjecture for the true order of the sum of the M\\\"{o}bius function, $M(x)$. This refined an earlier conjecture of Gonek from the 1990's. In this article we speculate on the true size of a large class of prime number error terms and present a general conjecture. This general conjecture includes both Montgomery's conjecture and the conjecture for $M(x)$ as special cases. Recently, Lamzouri (Springer volume: Essays in Analytic Number Theory, In Honor of Helmut Maier's 70th birthday) showed that an effective linear independence conjecture (ELI) for the zeros of the zeta function implies one of the inequalities in Montgomery's conjecture. In this article we adapt Lamzouri's method to show that a generalized effective linear independence (GELI) conjecture implies a lower bound for general prime number error terms. Furthermore, of independent interest, we prove an $L^2$ bound for almost periodic functions. This allows us to weaken significantly one of the conditions in Lamzouri's main result and also give an improvement of the main theorem in an article of Akbary-Ng-Shahabi (Q. J. Math. 65 (2014), no. 3)."
  },
  {
    "title": "Theta classes: generalized topological recursion, integrability and $\\mathcal{W}$-constraints",
    "abstract": "We study the intersection theory of the $\\Theta^{r,s}$-classes, where $r \\geq 2$ and $1 \\le s \\le r-1$, which are cohomological field theories obtained as the top degrees of Chiodo classes. We show that the recently introduced generalized topological recursion on the $(r,s)$ spectral curves computes the descendant integrals of the $\\Theta^{r,s}$-classes. As a consequence, we deduce that the descendant potential of the $\\Theta^{r,s}$-classes is a tau function of the $r$-KdV hierarchy, generalizing the Br\\'ezin--Gross--Witten tau function (the special case $r=2$, $s=1$). We also explicitly compute the $\\mathcal{W}$-constraints satisfied by the descendant potential, obtained as differential representations of the $\\mathcal{W}(\\mathfrak{gl}_r)$-algebra at self-dual level. This work extends previously known results on the Witten $r$-spin class, the $r$-spin $\\Theta$-classes (the case $s=r-1$), and the Norbury $\\Theta$-classes (the special case $r=2$, $s=1$)."
  },
  {
    "title": "Quantum compressed sensing tomographic reconstruction algorithm",
    "abstract": "Computed tomography (CT) is a non-destructive technique for observing internal images and has proven highly valuable in medical diagnostics. Recent advances in quantum computing have begun to influence tomographic reconstruction techniques. The quantum tomographic reconstruction algorithm is less affected by artifacts or noise than classical algorithms by using the square function of the difference between pixels obtained by projecting CT images in quantum superposition states and pixels obtained from experimental data. In particular, by using quantum linear systems, a fast quadratic unconstrained binary optimization (QUBO) model formulation for quantum tomographic reconstruction is possible. In this paper, we formulate the QUBO model for quantum compressed sensing tomographic reconstruction, which is a linear combination of the QUBO model for quantum tomographic reconstruction and the QUBO model for total variation in quantum superposition-state CT images. In our experiments, we used sinograms obtained by using the Radon transform of Shepp-Logan images and body CT images. We evaluate the performance of the new algorithm by reconstructing CT images using a hybrid solver with the QUBO model computed from each sinogram. The new algorithm was able to obtain a solution within 5 projection images for 30 by 30 image samples and within 6 projection images for 60 by 60 image samples, reconstructing error-free CT images. We anticipate that quantum compressed sensing tomographic reconstruction algorithms could significantly reduce the total radiation dose when quantum computing performance advances."
  },
  {
    "title": "Multiclass threshold-based classification",
    "abstract": "In this paper, we introduce a threshold-based framework for multiclass classification that generalizes the standard argmax rule. This is done by replacing the probabilistic interpretation of softmax outputs with a geometric one on the multidimensional simplex, where the classification depends on a multidimensional threshold. This change of perspective enables for any trained classification network an a posteriori optimization of the classification score by means of threshold tuning, as usually carried out in the binary setting. This allows a further refinement of the prediction capability of any network. Moreover, this multidimensional threshold-based setting makes it possible to define score-oriented losses, which are based on the interpretation of the threshold as a random variable. Our experiments show that the multidimensional threshold tuning yields consistent performance improvements across various networks and datasets, and that the proposed multiclass score-oriented losses are competitive with standard loss functions, resembling the advantages observed in the binary case."
  },
  {
    "title": "Multi-Fidelity Bayesian Optimization for Nash Equilibria with Black-Box Utilities",
    "abstract": "Modern open and softwarized systems -- such as O-RAN telecom networks and cloud computing platforms -- host independently developed applications with distinct, and potentially conflicting, objectives. Coordinating the behavior of such applications to ensure stable system operation poses significant challenges, especially when each application's utility is accessible only via costly, black-box evaluations. In this paper, we consider a centralized optimization framework in which a system controller suggests joint configurations to multiple strategic players, representing different applications, with the goal of aligning their incentives toward a stable outcome. To model this interaction, we formulate a Stackelberg game in which the central optimizer lacks access to analytical utility functions and instead must learn them through sequential, multi-fidelity evaluations. To address this challenge, we propose MF-UCB-PNE, a novel multi-fidelity Bayesian optimization strategy that leverages a budget-constrained sampling process to approximate pure Nash equilibrium (PNE) solutions. MF-UCB-PNE systematically balances exploration across low-cost approximations with high-fidelity exploitation steps, enabling efficient convergence to incentive-compatible configurations. We provide theoretical and empirical insights into the trade-offs between query cost and equilibrium accuracy, demonstrating the effectiveness of MF-UCB-PNE in identifying effective equilibrium solutions under limited cost budgets."
  },
  {
    "title": "Metastability for the Curie-Weiss-Potts model with unbounded random interactions",
    "abstract": "We analyse the metastable behaviour of the disordered Curie-Weiss-Potts (DCWP) model subject to a Glauber dynamics. The model is a randomly disordered version of the mean-field $q$-spin Potts model (CWP), where the interaction coefficients between spins are general independent random variables. These random variables are chosen to have fixed mean (for simplicity taken to be $1$) and well defined cumulant generating function, with a fixed distribution not depending on the number of particles. The system evolves as a discrete-time Markov chain with single spin flip Metropolis dynamics at finite inverse temperature $\\beta$. We provide a comparison of the metastable behaviour of the CWP and DCWP models, when $N \\to \\infty$. First, we establish the metastability of the CWP model and, using this result, prove metastability for the DCWP model (with high probability). We then determine the ratio between the metastable transition time for the DCWP model and the corresponding time for the CWP model. Specifically, we derive the asymptotic tail behavior and moments of this ratio. Our proof combines the potential-theoretic approach to metastability with concentration of measure techniques, the latter adapted to our specific context."
  },
  {
    "title": "Dimensionality-dependent electronic and vibrational dynamics in low-dimensional organic-inorganic tin halides",
    "abstract": "Photo-induced dynamics of electronic processes in materials are driven by the coupling between electronic and nuclear degrees of freedom. Here we construct 1D and 2D organic-inorganic tin halides to investigate the functional role of dimensionality to exciton-phonon coupling (EPC) and exciton self-trapping. The results show that the 1D system has strong EPC leading to excitation-independent self-trapped exciton (STE) emission, while the 2D system exhibits over ten times weaker EPC resulting in free exciton emission. By performing femtosecond transient absorption experiments, we directly resolve the room-temperature vibrational wavepackets in the 1D system, some of which propagate along the STE potential energy surface. A combination of wagging and asymmetric stretching motions (~106 cm-1) in tin iodide is identified as such a mode inducing exciton self-trapping. While no room-temperature wavepackets are observed in the 2D system. These findings uncover the interplay between the dimensionality-dependent EPC and electronic/nuclear dynamics, offering constructive guidance to develop multifunctional organic-inorganic metal halides."
  },
  {
    "title": "Parametric Model Order Reduction by Box Clustering with Applications in Mechatronic Systems",
    "abstract": "High temperatures and structural deformations can compromise the functionality and reliability of new components for mechatronic systems. Therefore, high-fidelity simulations (HFS) are employed during the design process, as they enable a detailed analysis of the thermal and structural behavior of the system. However, such simulations are both computationally expensive and tedious, particularly during iterative optimization procedures. Establishing a parametric reduced order model (pROM) can accelerate the design's optimization if the model can accurately predict the behavior over a wide range of material and geometric properties. However, many existing methods exhibit limitations when applied to wide design ranges. In this work, we introduce the parametric Box Reduction (pBR) method, a matrix interpolation technique that minimizes the non-physical influence of training points due to the large parameter ranges. For this purpose, we define a new interpolation function that computes a local weight for each design variable and integrates them into the global function. Furthermore, we develop an intuitive clustering technique to select the training points for the model, avoiding numerical artifacts from distant points. Additionally, these two strategies do not require normalizing the parameter space and handle every property equally. The effectiveness of the pBR method is validated through two physical applications: structural deformation of a cantilever Timoshenko beam and heat transfer of a power module of a power converter. The results demonstrate that the pBR approach can accurately capture the behavior of mechatronic components across large parameter ranges without sacrificing computational efficiency."
  },
  {
    "title": "Emergent Thermalization Thresholds in Unitary Dynamics of Inhomogeneously Disordered Systems",
    "abstract": "Inspired by the avalanche scenario for many-body localization (MBL) instability, we reverse the conventional set-up and ask whether a large weakly-disordered chain can thermalize a smaller, strongly-disordered chain when the composite system evolves unitarily. Using transport as a dynamical probe, we identify three distinct thermalization regimes as a function of the disorder strength of the smaller chain: (i) complete thermalization with self-averaging at weak disorder, (ii) realization-dependent thermalization with strong sample-to-sample fluctuations at intermediate disorder, and (iii) absence of thermalization at strong disorder. We find that the non-self-averaging regime broadens with the size of the weakly-disordered chain, revealing a nuanced interplay between disorder and system size. These results highlight how inhomogeneous disorder can induce emergent thermalization thresholds in closed quantum systems, providing direct access to disorder regimes where thermalization or its absence can be reliably observed."
  },
  {
    "title": "Depolarization of synchrotron radiation of a relativistic electron beam",
    "abstract": "We present a theoretical study on the radiative self-polarization of a high-energy electron beam propagating perpendicular to a strong magnetic field. Recently, a similar setup has been proposed as a source of polarized electron and photon beams. We focus on the dependence of electron and radiation polarization on the dimensionless parameter $\\varepsilon$, which is proportional to the product of electron energy and magnetic field strength. The numerical solution of the balance equation shows that the resulting electron beam polarization increases rapidly as a function of $\\varepsilon$ for $\\varepsilon \\ll 1$ and saturates at a value of approximately $-0.8$. If $\\varepsilon \\gg 1$, the rate of self-polarization decreases significantly. At the same time, a substantial or nearly complete depolarization of synchrotron radiation is observed, particularly for an electron beam with spins initially aligned parallel to the field."
  },
  {
    "title": "LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios",
    "abstract": "Ensuring the safety and robustness of autonomous driving systems necessitates a comprehensive evaluation in safety-critical scenarios. However, these safety-critical scenarios are rare and difficult to collect from real-world driving data, posing significant challenges to effectively assessing the performance of autonomous vehicles. Typical existing methods often suffer from limited controllability and lack user-friendliness, as extensive expert knowledge is essentially required. To address these challenges, we propose LD-Scene, a novel framework that integrates Large Language Models (LLMs) with Latent Diffusion Models (LDMs) for user-controllable adversarial scenario generation through natural language. Our approach comprises an LDM that captures realistic driving trajectory distributions and an LLM-based guidance module that translates user queries into adversarial loss functions, facilitating the generation of scenarios aligned with user queries. The guidance module integrates an LLM-based Chain-of-Thought (CoT) code generator and an LLM-based code debugger, enhancing the controllability and robustness in generating guidance functions. Extensive experiments conducted on the nuScenes dataset demonstrate that LD-Scene achieves state-of-the-art performance in generating realistic, diverse, and effective adversarial scenarios. Furthermore, our framework provides fine-grained control over adversarial behaviors, thereby facilitating more effective testing tailored to specific driving scenarios."
  },
  {
    "title": "Unique global solution of an integral-differential equation of Footloose Entrepreneur model in new economic geography",
    "abstract": "This paper studies the Footloose Entrepreneur model in new economic geography in continuous space. In an appropriate function space, the model is formulated as an initial value problem for an infinite-dimensional ordinary differential equation. A unique global solution is constructed based on the Banach fixed point theorem. The stability of a homogeneous stationary solution is then investigated and numerical simulations of the asymptotic behavior of the solution are performed. Numerical solutions starting near the unstable homogeneous stationary solution converge to spike-shaped stationary solutions, and the number of spikes decreases with decreasing transport costs and strengthening preference for variety."
  },
  {
    "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization",
    "abstract": "While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%."
  },
  {
    "title": "The Parameter Dependence of $\\mathbf{n_{s}}$ and $\\mathbf{r}$ of the Scalar Power Spectrum during Single-Field Slow-Roll Inflation: A Comparative Study of Inflationary Potentials",
    "abstract": "Inflation in cosmology is a specific stage preceding the Big Bang, aimed at solving both old background problems and new perturbation issues. Single-field inflation is a candidate to illustrate the picture of the initial universe, and various potential functions lead to different scenarios during the inflationary stage. This paper introduces two essential parameters: the spectral index and the tensor-to-scalar ratio detected from the initial power spectrum, derived from the action of the scalar field and using approximation that the potential is flat. A brief overview of the origins of Starobinsky Inflation, Chaotic Inflation, Small Field Inflation, and Natural Inflation is also presented, along with their mathematical representations. Finally, the results derived from various inflation models regarding the index and ratio are tested using the Planck data, and the deviations in each model are analyzed."
  }
]